
# NewWordDiscovery
~~~~~~~~~~~~~~~~~~~~~~~~
基于机器学习的方式使用多进程进行新词发现。
通过分类模型找到弹幕候选词特征与弹幕新词之间的隐含关系。
首先需要对弹幕语料进行预处理，通过 N 元切分得到弹幕候选词集。
然后提取弹幕候选词的特征作为分类模型的输入特征并进行样本学习。
通过不断增加标注样本并判断模型是否处于稳定，最后用稳定的模型来预测弹幕新词。
    核心思想：
        互信息 
        假设W为候选词，Wm和Wn分别代表构成W的左右两个子串，P(W)为W的出现概率，P(Wm)，P(Wn)为Wm和Wn的出现概率
        Mut(W)=log (P(W))/(P(W_m )*P(W_n ))
            
        左信息熵
        设L是W的左邻接字集合，其左邻接字l(l∈L)在所有W的左邻接组合中的出现概率为P_L(l)，则W的左信息熵如公式：
        Ent_L(W)=-∑_(l∈L)P_L(l)*log(P_L(l))
        同理，设R是W的右邻接字集合，其右邻接字r(r∈R)在所有W的右邻接组合中的出现概率为P_R(r)右信息熵如公式：
        Ent_R(W)=-∑_(r∈R)P_R(r)*log(P_R(r)) 
        
        覆盖密度
        假设候选词W在该视频的F条弹幕中出现，视频时长为T秒，则W在这一部视频中的覆盖密度如下式所示：
        Den(W)=F/T       

        复用率
        候选词W的复用率为W在一部视频的所有弹幕中的连续复用总次数。假设W在n条弹幕中出现复用，每条弹幕中重复出现ki次，则候选词复用率为
        Rep(W)=∑_(i=1)^n(ki-1) 

    通过设定最低词频、最低互信息、信息熵 来得到候选词，只有利用训练后的随机森林模型的到新词集
    p_min（最小概率或频数）、co_min（互信息）、h_max（最小左右信息熵） 
    越小查找到的词语数量越多

    若使用新的文本数据，请对应修改 get_corpus.py  下 get_word（） 函数中文本读取代码，返回文本数据迭代器

    主要参数：

    --file_name  训练文件名称

    【常用设置 重要参数】
    --n_gram  提取的新词长度  默认为5。 即超过5个字符的新词不再处理
    --p_min  词出现的最小概率 （p_min = 3 整数则为频数， p_min = 0.00001 浮点数则为概率） 【dytpe: int, default 0.00001 】
    --co_min  最小凝固度，只有超过最小凝固度才继续判断自由度并进入下一步搜索  【dytpe: int, default 100】
    --h_min   最大自由度，若小于最大自由度，则认为词组不完整，为大词组中的一小部分 【dytpe: int, default 1.2】
    --
    
    --level_s  CMD界面窗口显示日志级别. 【默认为INFO】
    --level_f  日志文件记录级别. 【默认为INFO】

    --batch_len  批次计算的文本字符串长度 。【 字符串长度减少可降低占用内存，默认1000000个字符就进入统计计算】
    --top_n  保存 top_n 个词组 参数设置越大，结果准确度越高，内存也增加, 在硬件配置允许的条件下应尽量调高 【默认 1000000】
    
    [参考资料: 基于信息熵的无字典分词算法](http://www.cnblogs.com/bigdatafly/p/5014597.html)
-----------------------------------------

